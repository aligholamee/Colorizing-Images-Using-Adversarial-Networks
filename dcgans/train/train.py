from random import randint, shuffle
import matplotlib.pyplot as plt
import cPickle as pickle
import numpy as np
import gzip
import sys
import os

sys.path.insert(0, '../architecture/')
sys.path.insert(0, '../../ops/')
import architecture as arch
from mnist import mnist

def train(checkpoint_dir, batch_size, d_learning_rate, g_learning_rate):


   train_images = np.asarray(train_images)

   # keep the step and epoch numbers in the graph so they are ready when reloading a model
   global_step = tf.Variable(0, name='global_step', trainable=False)
   epoch_num   = tf.Variable(0, name='epoch_num', trainable=False)

   # placeholder for the real images
   images = tf.placeholder(tf.float32, shape=(batch_size, 28, 28, 1), name='real_images')

   # placeholder for the images generated by G
   sample_images = tf.placeholder(tf.float32, shape=(batch_size, 28, 28, 1), name='sample_images')

   # placeholder for the random array z being fed into G
   z = tf.placeholder(tf.float32, shape=(batch_size, 100), name='z')

   # generate an image 'G' using the random vector z
   G = arch.generator(z, batch_size)

   # send real images to D
   D, D_logits = arch.discriminator(images, batch_size)

   # send images generated by G to D
   D_, D_logits_ = arch.discriminator(G, batch_size, reuse=True)

   # compute loss for D on real images
   d_loss_real = arch.loss(D_logits, tf.ones_like(D))

   # compute loss for D on fake images
   d_loss_fake = arch.loss(D_logits_, tf.zeros_like(D_))

   # combine losses
   d_loss = d_loss_real + d_loss_fake

   # compute loss for G given the decision made by D on the generated images
   g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(D_logits_, tf.ones_like(D_)))

   # initialize variables that are learned
   t_vars = tf.trainable_variables()

   # seperate out variables for respective graphs. These names correspond to names given in the architecture
   d_vars = [var for var in t_vars if 'd_' in var.name]
   g_vars = [var for var in t_vars if 'g_' in var.name]

   # run optimizer
   d_optim = tf.train.AdamOptimizer(d_learning_rate, beta1=0.5).minimize(d_loss, var_list=d_vars)
   g_optim = tf.train.AdamOptimizer(g_learning_rate, beta1=0.5).minimize(g_loss, var_list=g_vars)

   # Tensorboard summaries
   images_summary        = tf.image_summary('images', images, max_images=500)
   sample_images_summary = tf.image_summary('sample_images', images, max_images=500)
   tf.scalar_summary('d_loss_real', d_loss_real)
   tf.scalar_summary('d_loss_fake', d_loss_fake)
   tf.scalar_summary('d_loss', d_loss)
   tf.scalar_summary('g_loss', g_loss)
   tf.histogram_summary('d', D)
   tf.histogram_summary('d_', D_)
   tf.histogram_summary('g', G)
   summary_op = tf.merge_all_summaries()

   # start a session and initialize all variables
   sess = tf.Session()
   sess.run(tf.initialize_all_variables())

   # create a saver object so we can store the model
   saver = tf.train.Saver()

   step = 0

   # create the directory to save model if it isn't already made
   try:
      os.mkdir(checkpoint_dir)
   except:
      pass

   ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
   if ckpt and ckpt.model_checkpoint_path:
      print "Restoring previous model..."
      try:
         saver.restore(sess, ckpt.model_checkpoint_path)
         print "Model restored"
      except:
         print "Could not restore model"
         pass

   while True:

      # get a random position to start at in the list of images that ensures you won't overshoot the end
      random_batch_start = randint(0,len(train_images)-batch_size)
      batch_images = train_images[random_batch_start:random_batch_start+batch_size]

      # generate random z vector for feeding into G
      batch_z = np.random.uniform(-1, 1, [batch_size, 100]).astype(np.float32)

      _, fake_d_loss, real_d_loss = sess.run([d_optim, d_loss_fake, d_loss_real], feed_dict={images: batch_images, z: batch_z})
      
      _, loss_g, gen_images = sess.run([g_optim, g_loss, G], feed_dict={z: batch_z})
      _, loss_g, gen_images = sess.run([g_optim, g_loss, G], feed_dict={z: batch_z})

      # save the model every 100 steps and also save for tensorboard (not yet written)
      if step % 100 == 0:
         try:
            os.system('rm images/*')
         except:
            pass
         print 'Saving model'
         saver.save(sess, checkpoint_dir+'checkpoint_', global_step=global_step)
         print 'Writing out generated images'
         shuffle(gen_images)
         i = 0
         for img in gen_images:
            img = np.squeeze(img)
            plt.imsave('images/image_step-'+str(step)+'_'+str(i)+'.png', img)
            i += 1
            if i == 200:
               break
      step += 1

      print 'step: ', step
      print 'd_loss: ', real_d_loss+fake_d_loss
      print 'g_loss: ', loss_g
      print


def main(argv=None):
   checkpoint_dir = sys.argv[1]

   g_learning_rate = 1e-4
   d_learning_rate = 1e-4
   batch_size = 128

   if checkpoint_dir[-1] != '/':
      checkpoint_dir+='/'

   print
   print 'checkpoint_dir:  ', checkpoint_dir
   print 'batch_size:      ', batch_size
   print 'd_learning_rate: ', d_learning_rate
   print 'g_learning_rate: ', g_learning_rate
   print

   train(checkpoint_dir, batch_size, d_learning_rate, g_learning_rate)


if __name__ == '__main__':
   if sys.argv[1] == '--help' or sys.argv[1] == '-h':
      print
      print 'python train.py [checkpoint_dir] [batch_size]'
      print
      print 'checkpoint_dir <str> [path to save model]'
      print 'batch_size     <int> [batch size]'
      print
      exit()

   import tensorflow as tf
   tf.app.run()


